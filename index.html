---
layout: single
author_profile: true
---

<p>
    I'm a 2nd year Ph.D. student from <a href="https://vast.cs.ucla.edu/">UCLA VAST Lab</a>, advised by <a href="https://vast.cs.ucla.edu/people/faculty/jason-cong">Prof. Jason Cong</a>. 
    My major research focus is on improving algorithms and hardware/system designs for efficient and high-quality AI inference. Specifically, I'm working on developing
    <ul>
        <li>New language model architecture or system of models with high generation quality and hardware-friendly</li>
        <li>New architecture designs paradigms and frameworks to deploy our designed models on heterogeneous resources efficiently (DSP, LUT, SRAM, SIMD cores, etc.)</li>
    </ul>
    The ultimate goal is to enable personalized AI agents on resource-constrained devices, which requires low power consumption, low latency, and high generation quality at the same time.
</p>

<h1>News</h1>

<ul>
    <li>2025.05.05: Our paper InTAR is presented at <a href="https://www.fccm.org">FCCM 2025</a>!</li>
    <li>2025.04.30: Our paper HMT is presented at <a href="https://2025.naacl.org/">NAACL 2025</a>!</li>
</ul>

<h1>Education</h1>

<h1>Work Experience</h1>

<h1>Publications</h1>

<p>
    <strong>
        <a href="https://arxiv.org/abs/2502.08807">
            InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs
        </a>
    </strong>
    <a href="https://github.com/OswaldHe/InTAR">
        <i class="fab fa-fw fa-github" aria-hidden="true"></i>
    </a>
    <br>
    <i>TL;DR: A novel accelerator design methodology for DNNs that improves on-chip data utilization with auto-reconfiguration.</i>
    <br>
    <strong>Zifan He</strong>, Anderson Truong, Yingqi Cao, Jason Cong
    <br>
    <span style="color:gray">
        <i>FCCM</i>, 2025 
    </span>
</p>

<p>
    <strong>
        <a href="https://aclanthology.org/2025.naacl-long.410/">
            HMT: Hierarchical Memory Transformer for Efficient Long Context Language Processing
        </a>
    </strong>
    <a href="https://github.com/OswaldHe/HMT-pytorch">
        <i class="fab fa-fw fa-github" aria-hidden="true"></i>
    </a>
    <br>
    <i>TL;DR: A novel language model architecture for efficient long-context inputs, achieving a comparable or superior generation quality to long-context LLMs with 2-57x fewer parameters and 2.5-116x less inference memory. </i>
    <br>
    <strong>Zifan He</strong>, Yingqi Cao, Zongyue Qin, Neha Prakriya, Yizhou Sun, Jason Cong
    <br>
    <span style="color:gray">
        <i>NAACL</i>, 2025
    </span>
</p>

<p>
    <strong>
        <a href="https://dl.acm.org/doi/abs/10.1145/3626202.3637568">
            LevelST: Stream-based Accelerator for Sparse Triangular Solver
        </a>
    </strong>
    <a href="https://github.com/OswaldHe/LevelST">
        <i class="fab fa-fw fa-github" aria-hidden="true"></i>
    </a>
    <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#available">
        <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" 
             style="vertical-align:top;" 
             width="3%">
    </a>
    <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#functional">
        <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" 
             style="vertical-align:top;" 
             width="3%">
    </a>
    <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#reproduced">
        <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg" 
             style="vertical-align:top;" 
             width="3%">
    </a>
    <br>
    <i>TL;DR: A novel stream-based accelerator for sparse triangular solver on FPGA, with 2.65x speedup over GPU. </i>
    <br>
    <strong>Zifan He</strong>, Linghao Song, Robert F. Lucas, Jason Cong
    <br>
    <span style="color:gray">
        <i>FPGA</i>, 2024 
    </span>
</p>


<h1>Teaching</h1>

<h1>Professional Services</h1>

<h1>Awards & Talks</h1>