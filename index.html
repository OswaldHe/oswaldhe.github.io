---
layout: single
author_profile: true
---

<p>
    I'm a 2nd year Ph.D. student from <a href="https://vast.cs.ucla.edu/">UCLA VAST Lab</a>, advised by <a href="https://vast.cs.ucla.edu/people/faculty/jason-cong">Prof. Jason Cong</a>. 
    My major research focus is on improving algorithms and hardware/system designs for efficient and high-quality AI inference. Specifically, I'm working on developing
    <ul>
        <li>New language model architecture or system of models with high generation quality and hardware-friendly</li>
        <li>New architecture designs paradigms and frameworks to deploy our designed models on heterogeneous resources efficiently (DSP, LUT, SRAM, SIMD cores, etc.)</li>
    </ul>
    The ultimate goal is to enable personalized AI agents on resource-constrained devices, which requires low power consumption, low latency, and high generation quality at the same time.
</p>

<h1>News</h1>

<ul>
    <li>2025.05.30: I will join Microsoft Research at Redmond, WA as a student research intern this summer!</li>
    <li>2025.05.12: I'm honored to be selected as the <a href="https://www.amd-haccs.io/awards.html">AMD HACC Outstanding Researcher</a>!</li>
    <li>2025.05.05: Our paper InTAR is presented at <a href="https://www.fccm.org">FCCM 2025</a>!</li>
    <li>2025.04.30: Our paper HMT is presented at <a href="https://2025.naacl.org/">NAACL 2025</a>!</li>
</ul>

<h1>Education</h1>

<p>
<strong>
    University of California, Los Angeles (UCLA)
</strong>
<br>
Ph.D. in Computer Science
<br>
Sept. 2023 - Present
<br>
<br>
Accumulated GPA: 4.0/4.0
</p>

<p>
<strong>
    University of California, Los Angeles (UCLA)
</strong>
<br>
B. S. in Computer Science
<br>
B. S. in Applied Mathematics
<br>
Sept. 2019 - June 2023
<br>
<br>
Accumulated GPA: 4.0/4.0
</p>

<h1>Work Experience</h1>

<p>
<strong>
    Microsoft Research
</strong>
<br>
Student Research Intern, AI Acceleration and Hardware Design 
<br>
Jun. 2025 - Sept. 2025
</p>

<p>
<strong>
    Meta
</strong>
<br>
Production Engineer Internship, Meta Fintech Payment 
<br>
Jun. 2022 - Sept. 2022
</p>

<h1>Publications</h1>

<p>
    <strong>
        <a href="https://ieeexplore.ieee.org/document/11008978">
            InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs
        </a>
    </strong>
    <a href="https://github.com/OswaldHe/InTAR">
        <i class="fab fa-fw fa-github" aria-hidden="true"></i>
    </a>
    <br>
    <i>TL;DR: A novel accelerator design methodology for DNNs that improves on-chip data utilization with auto-reconfiguration.</i>
    <br>
    <strong>Zifan He</strong>, Anderson Truong, Yingqi Cao, Jason Cong
    <br>
    <span style="color:gray">
        <i>FCCM</i>, 2025 
    </span>
</p>

<p>
    <strong>
        <a href="https://arxiv.org/abs/2407.09722">
            Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM Inference
        </a>
    </strong>
    <br>
    Zongyue Qin, Ziniu Hu, <strong>Zifan He</strong>, Neha Prakriya, Jason Cong, Yizhou Sun
    <br>
    <span style="color:gray">
        <i>ICLR</i>, 2025 
    </span>
</p>

<p>
    <strong>
        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/34690">
            Dynamic-Width Speculative Beam Decoding for LLM Inference 
        </a>
    </strong>
    <br>
    Zongyue Qin, <strong>Zifan He</strong>, Neha Prakriya, Jason Cong, Yizhou Sun
    <br>
    <span style="color:gray">
        <i>AAAI</i>, 2025 
    </span>
</p>

<p>
    <strong>
        <a href="https://aclanthology.org/2025.naacl-long.410/">
            HMT: Hierarchical Memory Transformer for Efficient Long Context Language Processing
        </a>
    </strong>
    <a href="https://github.com/OswaldHe/HMT-pytorch">
        <i class="fab fa-fw fa-github" aria-hidden="true"></i>
    </a>
    <br>
    <i>TL;DR: A novel language model architecture for efficient long-context inputs, achieving a comparable or superior generation quality to long-context LLMs with 2-57x fewer parameters and 2.5-116x less inference memory. </i>
    <br>
    <strong>Zifan He</strong>, Yingqi Cao, Zongyue Qin, Neha Prakriya, Yizhou Sun, Jason Cong
    <br>
    <span style="color:gray">
        <i>NAACL</i>, 2025
    </span>
</p>

<p>
    <strong>
        <a href="https://dl.acm.org/doi/abs/10.1145/3626202.3637568">
            LevelST: Stream-based Accelerator for Sparse Triangular Solver
        </a>
    </strong>
    <a href="https://github.com/OswaldHe/LevelST">
        <i class="fab fa-fw fa-github" aria-hidden="true"></i>
    </a>
    <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#available">
        <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" 
             style="vertical-align:top;" 
             width="3%">
    </a>
    <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#functional">
        <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" 
             style="vertical-align:top;" 
             width="3%">
    </a>
    <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current#reproduced">
        <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg" 
             style="vertical-align:top;" 
             width="3%">
    </a>
    <br>
    <i>TL;DR: A novel stream-based accelerator for sparse triangular solver on FPGA, with 2.65x speedup over GPU. </i>
    <br>
    <strong>Zifan He</strong>, Linghao Song, Robert F. Lucas, Jason Cong
    <br>
    <span style="color:gray">
        <i>FPGA</i>, 2024 
    </span>
</p>

<p>
    <strong>
        <a href="https://ieeexplore.ieee.org/abstract/document/10000846">
            Optimization of Assisted Search Over Server-Mediated Peer-to-peer Networks
        </a>
    </strong>
    <br>
    <strong>Zifan He</strong>, Leonard Kleinrock
    <br>
    <span style="color:gray">
        <i>GLOBECOM</i>, 2022
    </span>
</p>

<h1>Professional Services</h1>

<ul>
    <li>
        <strong>Conference Reviewer:</strong> FPGA 2024, FPGA 2025
    </li>
    <li>
        <strong>Journal Reviewer:</strong> IEEE Transactions on Parallel and Distributed Systems (TPDS)
    </li>
    <li>
        <strong>Artifact Evaluator:</strong> FPGA 2024
    </li>
</ul>

<h1>Awards</h1>

<ul>
    <li>
        AMD HACC Outstanding Researcher Award 2024
    </li>
    <li>
        UCLA DGE Dean's Scholar Award 2023
    </li>
    <li>
        UCLA Outstanding CS Undergraduate Award 2023
    </li>
    <li>
        UCLA Internet Research Initiative Prize 2022
    </li>
</ul>